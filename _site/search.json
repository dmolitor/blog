[
  {
    "objectID": "posts/causal-inf-high-dim/index.html",
    "href": "posts/causal-inf-high-dim/index.html",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "",
    "text": "library(dplyr)\nlibrary(glmnet)\nlibrary(here)\nlibrary(Matrix)\nlibrary(readr)\n\n\n\n\n\nThis will be replicating (to a small degree), Chernozhukov et al.’s work on estimating causal effects in high-dimensional settings. This work demonstrates how to estimate treatment effects including LATE (in an Instrumental Variables setting) and ATE among others, as well as constructing honest confidence bands. They demonstrate this within a specific empirical setting. This setting is a prior study which uses Instrumental Variables estimation to quantify the effect of 401(k) participation on accumulated assets. The instrument in this setting is 401(k) eligibility which is arguably exogenous after conditioning on income and other related confounders."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#study-data",
    "href": "posts/causal-inf-high-dim/index.html#study-data",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Study Data",
    "text": "Study Data\n\nFirst, I will import the data that this study is based on.\n\niv_data <- read_tsv(here(\"posts/causal-inf-high-dim/data/high-dim-iv.dat\"))\n\n# Variable transformations\niv_data <- iv_data |>\n  mutate(\n    age = (age - 25)/(64 - 25),\n    inc = (inc + 2652)/(242124 + 2652),\n    fsize = fsize/13,\n    educ = educ/18\n  )\n\nThe primary variables of interest are our continuous outcome \\(Y=\\) Total Assets, our binary instrument \\(Z=\\) 401(k) Eligibility, and our binary treatment \\(D=\\) 401(k) Participation. The definitions of all the rest of these variables are kind of obscure, and I’m not sure what they all are exactly. However, they define their specification precisely so I’m simply going to mirror what they use in their paper."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#model-specification",
    "href": "posts/causal-inf-high-dim/index.html#model-specification",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Model Specification",
    "text": "Model Specification\n\nThe specification of the full set of controls they construct is defined below as iv_control_spec. It’s a pretty hairy model spec! Let’s create the input matrix and check it’s dimensions.\n\niv_control_spec <- ~ (\n  (\n    marr + twoearn + db + pira + hown + age + I(age^2) + I(age^3)\n    + educ + I(educ^2) + fsize + I(fsize^2)\n  )^2\n  * (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))\n  + (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))^2\n) - 1\niv_X <- sparse.model.matrix(iv_control_spec, data = iv_data)\n\n\n\nN Observations: 9915; N Confounders: 738\n\n\nWhile this isn’t quite the full set of potential controls considered in the paper, it should be close enough to get the point across."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#first-stage-estimates",
    "href": "posts/causal-inf-high-dim/index.html#first-stage-estimates",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "First-Stage Estimates",
    "text": "First-Stage Estimates\n\nConditional Expected Outcomes\n\nIn order to calculate the LATE in our IV framework, we will estimate the following expected values: \\(E[Y|Z=0,X]\\), \\(E[Y|Z=1,X]\\), \\(E[D|Z=1,X]\\), and \\(E[Z|X]\\) using post-LASSO estimates for each of these values. Let’s get to estimating! As in the paper, we will estimate using LASSO with a pre-determined, data-driven choice of regularization parameter, \\(\\lambda\\).\n\nN <- nrow(iv_X)\nP <- ncol(iv_X)\n\n# Data-driven regularization parameters\nlambda <- 2.2 * sqrt(N) * qnorm(1 - (0.1/log(N))/(2 * (2 * P)))\nlogit_lambda <- lambda/(2 * N)\n\n# Estimate models where instrument == 0\n\n## Outcome Post-LASSO model\nid_z0 <- iv_data$e401 == 0\ney_z0 <- glmnet(x = iv_X[id_z0, ], y = iv_data$tw[id_z0], lambda = lambda)\ney_z0_selected <- names((c <- coef(ey_z0))[(drop(c) != 0), ])\ney_z0_post_data <- cbind(tw = iv_data$tw[id_z0], as.matrix(iv_X[id_z0, ]))\ney_z0_post_form <- paste(\"tw ~\", paste0(ey_z0_selected[-1], collapse = \"+\"))\ney_z0_lm <- lm(formula(ey_z0_post_form), data = as.data.frame(ey_z0_post_data))\n\n## Treatment Post-LASSO model - not needed (E[D] = 0, since D = 1 iff Z = 1)\n\n# Estimate models where instrument == 1\n\n## Outcome Post-LASSO model\nid_z1 <- iv_data$e401 == 1\ney_z1 <- glmnet(x = iv_X[id_z1, ], y = iv_data$tw[id_z1], lambda = lambda)\ney_z1_selected <- names((c <- coef(ey_z1))[(drop(c) != 0), ])\ney_z1_post_data <- cbind(tw = iv_data$tw[id_z1], as.matrix(iv_X[id_z1, ]))\ney_z1_post_form <- paste(\"tw ~\", paste0(ey_z1_selected[-1], collapse = \"+\"))\ney_z1_lm <- lm(formula(ey_z1_post_form), data = as.data.frame(ey_z1_post_data))\n\n## Treatment Post-LASSO model\ned_z1 <- glmnet(\n  x = iv_X[id_z1, ],\n  y = iv_data$p401[id_z1],\n  family = \"binomial\",\n  lambda = logit_lambda\n)\ned_z1_selected <- names((c <- coef(ed_z1))[(drop(c) != 0), ])\ned_z1_post_data <- cbind(p401 = iv_data$p401[id_z1], as.matrix(iv_X[id_z1, ]))\ned_z1_post_form <- paste(\"p401 ~\", paste0(ed_z1_selected[-1], collapse = \"+\"))\ned_z1_lm <- glm(\n  formula(ed_z1_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ed_z1_post_data)\n)\n\n# Estimate instrument as a function of X; Post-LASSO\nez <- glmnet(\n  x = iv_X,\n  y = iv_data$e401,\n  family = \"binomial\",\n  lambda = logit_lambda\n)\nez_selected <- names((c <- coef(ez))[(drop(c) != 0), ])\nez_post_data <- cbind(e401 = iv_data$e401, as.matrix(iv_X))\nez_post_form <- paste(\"e401 ~\", paste0(ez_selected[-1], collapse = \"+\"))\nez_lm <- glm(\n  formula(ez_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ez_post_data)\n)\n\n\n\n\nCalculate LATE\n\nNow, that we’ve estimated models for the expected value of \\(Y\\) and \\(D\\) under the different values of our instrument \\(Z\\), let’s create a data.frame that has the estimated expected values of these variables for every observation. As is standard (and implemented in the paper), we will trim observations to ensure that estimated propensities of our instrument are bounded away from \\({0, 1}\\).\n\nprediction_data <- as.data.frame(as.matrix(iv_X))\niv_expected_values <- data.frame(\n  y = iv_data$tw,\n  d = iv_data$p401,\n  z = iv_data$e401,\n  ey_z0 = predict(ey_z0_lm, prediction_data),\n  ey_z1 = predict(ey_z0_lm, prediction_data),\n  ed_z0 = 0,\n  ed_z1 = predict(ed_z1_lm, prediction_data, type = \"response\"),\n  ez = predict(ez_lm, prediction_data, type = \"response\")\n)\n\n# Trim instrument propensity scores -- No observations are dropped here\niv_expected_values <- iv_expected_values |>\n  filter(ez >= 1e-12 & ez <= (1 - 1e-12))\n\n# Estimate LATE plug-in values\niv_expected_values <- iv_expected_values |>\n  mutate(\n    ay_1 = z*(y - ey_z1)/ez + ey_z1,\n    ay_0 = (1 - z)*(y - ey_z0)/(1 - ez) + ey_z0,\n    ad_1 = z*(d - ed_z1)/ez + ed_z1,\n    ad_0 = 0,\n    LATE = (ay_1 - ay_0)/(ad_1 - ad_0)\n  )\n\n\n\n\nConfidence via Bootstrap\n\nNow that we’ve estimated the plug-in values, let’s calculate the LATE and generate a confidence interval using the described multiplier bootstrap.\n\n# Calculate LATE\nmean_ay_1 <- mean(iv_expected_values$ay_1)\nmean_ay_0 <- mean(iv_expected_values$ay_0)\nmean_ad_1 <- mean(iv_expected_values$ad_1)\nmean_ad_0 <- mean(iv_expected_values$ad_0)\nLATE <- (mean_ay_1 - mean_ay_0)/(mean_ad_1 - mean_ad_0)\n\n# Confidence intervals: both analytic and bootstrap\nanalytic_se <- sqrt(\n  (1/(nrow(iv_expected_values) - 1))\n  * sum(\n    (\n      (iv_expected_values$ay_1 - iv_expected_values$ay_0)\n      /(mean_ad_1 - mean_ad_0)\n      - LATE\n    )^2\n  )\n  /nrow(iv_expected_values)\n)\n\n# Function to generate multiplier weights\nmw <- function(n) {\n  1 + rnorm(n)/sqrt(2) + (rnorm(n)^2 - 1)/2\n}\n\nbootstrap_LATEs <- vapply(\n  1:500,\n  function(i) {\n    weights <- mw(nrow(iv_expected_values))\n    (\n      mean((iv_expected_values$ay_1 - iv_expected_values$ay_0)*weights)\n      /mean((iv_expected_values$ad_1 - iv_expected_values$ad_0)*weights)\n    )\n  },\n  numeric(1)\n)\nbootstrap_se <- (\n  (quantile(bootstrap_LATEs, .75) - quantile(bootstrap_LATEs, .25))\n  / (qnorm(.75) - qnorm(.25))\n)\n\n\n\nLATE: 8391.53 (3305.03) {3098.99}"
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#conclusion",
    "href": "posts/causal-inf-high-dim/index.html#conclusion",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nAnd voila, we’ve estimated the LATE using IV estimation in a high-dimensional setting! A specific, but very useful, case of this general framework is when we want to directly estimate the effect of a treatment variable that is conditionally exogenous. In that case, we can execute the algorithm shown above, but setting \\(Z = D\\). Other than that, everything is exactly the same.\n\nHDM Package\nIf you want a quick and easy implementation for these methods, check out the hdm package. The package is relatively easy-to-follow, and also works with sparse matrices right out of the box, which is nice. It’s not the most user-friendly package, but it seems to get the job done."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Snippets of Things",
    "section": "",
    "text": "A. Belloni, V. Chernozhukov, I. Fernandez-Val, C. Hansen\n\n\n\n\nML\n\n\nCausal Inference\n\n\n\n\nEstimators and honest confidence bands for a variety of treatment effects including local average (LATE) and average treatment effects (ATE) in data-rich environments.\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\nNo matching items"
  }
]