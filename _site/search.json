[
  {
    "objectID": "posts/dml/index.html",
    "href": "posts/dml/index.html",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "",
    "text": "Double/Debiased Machine Learning, proposed by Chernozhukov et al., tackles the problem of inference on some parameter, \\(\\theta_0\\), in the setting where there is a potentially high-dimensional set of control variables/confounders. To do so, this method harnesses generic ML algorithms which perform well in high-dimensional settings. DML can be applied to learn the ATE and LATE in a partially linear regression model, and partially linear instrumental variables model, respectively. It can also be used to estimate the ATE and LATE in the interactive model and interactive instrumental variables model, where treatment effects are fully heterogeneous.\nThis post is going to show how to implement estimation of the ATE and LATE in both the partially linear and interactive settings. This will be a pretty MVP (minimal …) implemented entirely in base R. I’ll use a couple other packages for the ML methods, but which models you choose to use are entirely separate from the general method outlined here."
  },
  {
    "objectID": "posts/dml/index.html#estimating-the-ate-of-401k-eligibility-on-net-financial-assets",
    "href": "posts/dml/index.html#estimating-the-ate-of-401k-eligibility-on-net-financial-assets",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "Estimating the ATE of 401(k) Eligibility on Net Financial Assets",
    "text": "Estimating the ATE of 401(k) Eligibility on Net Financial Assets\n\n\nIn the example in this paper, we use the same data as in Chernozhukov and Hansen (2004). We use net financial assets – defined as the sum of IRA balances, 401(k) balances, checking accounts, US saving bonds, other interest‐earning accounts in banks and other financial institutions, other interest‐earning assets (such as bonds held personally), stocks, and mutual funds less non‐mortgage debt – as the outcome variable, Y, in our analysis. Our treatment variable, D, is an indicator for being eligible to enroll in a 401(k) plan. The vector of raw covariates, X, consists of age, income, family size, years of education, a married indicator, a two‐earner status indicator, a defined benefit pension status indicator, an IRA participation indicator, and a home‐ownership indicator.\n\n\n\nPartially Linear Regression Model\n\nFirst we will calculate the ATE of 401(k) eligibility on net financial assets in the partially linear model.\nThe following function implements DML for the partially linear model.\n\n\ndml_plm <- function(X, Y, D, y_learner, d_learner, pred_fn_y, pred_fn_d = pred_fn_y) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Outcome prediction model\n    outcome_model <- y_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Residualize outcome\n    outcome_predictions <- pred_fn_y(outcome_model, X[idx_predict, , drop = FALSE])\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    treatment_resids <- (\n      # This is necessary to deal with cases where D is a factor\n      as.numeric(as.character(D[idx_predict])) - treatment_predictions\n    )\n\n    # Collect residuals\n    new_resids <- data.frame(\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n\n\nNow that we’ve defined the function, let’s estimate the ATE using our three different ML methods.\n\n\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_plm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_plm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_plm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# Estimate the ATE using OLS\nate_lasso <- lm_robust(y_resid ~ d_resid, first_stage_lasso, se_type = \"HC1\")\nate_enet <- lm_robust(y_resid ~ d_resid, first_stage_enet, se_type = \"HC1\")\nate_rf <- lm_robust(y_resid ~ d_resid, first_stage_rf, se_type = \"HC1\")\n\n\n\n\n\n\n\nLASSO\nElasticNet\nRandomForest\n\n\n\n\nEstimate\n9738.496\n9816.064\n8986.493\n\n\nStd. Error\n1372.968\n1412.982\n1274.455\n\n\nRMSE Y\n53255.882\n53939.421\n54589.826\n\n\nRMSE D\n0.444\n0.444\n0.447\n\n\n\n\n\n\n\nInteractive Regression Model\n\nNow, we’ll define the function to estimate the ATE in the fully heterogeneous model.\n\n\ndml_irm <- function(X,\n                    Y,\n                    D,\n                    y_learner,\n                    d_learner,\n                    pred_fn_y,\n                    pred_fn_d = pred_fn_y) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    numD <- as.numeric(as.character(D))\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    treatment_resids <- (numD[idx_predict] - treatment_predictions)\n    \n    # Outcome prediction model\n    XD <- cbind(\"D\" = numD, X)\n    outcome_model <- y_learner(\n      x = XD[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Outcome predictions under control\n    XD[, \"D\"] <- 0\n    outcome_preds_d0 <- pred_fn_y(outcome_model, XD[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions under treatment\n    XD[, \"D\"] <- 1\n    outcome_preds_d1 <- pred_fn_y(outcome_model, XD[idx_predict, , drop = FALSE])\n    outcome_predictions <- (\n      numD[idx_predict]*outcome_preds_d1\n      + (1 - numD[idx_predict])*outcome_preds_d0\n    )\n    \n    # Calculate individual level weights\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    inv_prop_scores <- (\n      numD[idx_predict]/treatment_predictions\n      - (1 - numD[idx_predict])/(1 - treatment_predictions)\n    )\n    expected_diff <- outcome_preds_d1 - outcome_preds_d0\n    effect_estimates <- expected_diff + inv_prop_scores*outcome_resids\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"effect_estimates\" = unname(effect_estimates),\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n\nLet’s estimate the ATE!\n\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_irm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_irm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_irm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# Estimate the ATE using OLS\nate_lasso <- lm_robust(effect_estimates ~ 1, first_stage_lasso, se_type = \"HC1\")\nate_enet <- lm_robust(effect_estimates ~ 1, first_stage_enet, se_type = \"HC1\")\nate_rf <- lm_robust(effect_estimates ~ 1, first_stage_rf, se_type = \"HC1\")\n\n\n\n\n\n\n\nLASSO\nElasticNet\nRandomForest\n\n\n\n\nEstimate\n9078.850\n9311.623\n8100.171\n\n\nStd. Error\n1412.741\n1462.927\n1149.504\n\n\nRMSE Y\n53085.113\n53732.086\n54427.880\n\n\nRMSE D\n0.444\n0.444\n0.447"
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html",
    "href": "posts/causal-inf-high-dim/index.html",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "",
    "text": "library(dplyr)\nlibrary(glmnet)\nlibrary(here)\nlibrary(Matrix)\nlibrary(readr)\n\n\n\n\n\nThis will be replicating (to a small degree), Chernozhukov et al.’s work on estimating causal effects in high-dimensional settings. This work demonstrates how to estimate treatment effects including LATE (in an Instrumental Variables setting) and ATE among others, as well as constructing honest confidence bands. They demonstrate this within a specific empirical setting. This setting is a prior study which uses Instrumental Variables estimation to quantify the effect of 401(k) participation on accumulated assets. The instrument in this setting is 401(k) eligibility which is arguably exogenous after conditioning on income and other related confounders."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#study-data",
    "href": "posts/causal-inf-high-dim/index.html#study-data",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Study Data",
    "text": "Study Data\n\nFirst, I will import the data that this study is based on.\n\niv_data <- read_tsv(here(\"posts/causal-inf-high-dim/data/high-dim-iv.dat\"))\n\n# Variable transformations\niv_data <- iv_data |>\n  mutate(\n    age = (age - 25)/(64 - 25),\n    inc = (inc + 2652)/(242124 + 2652),\n    fsize = fsize/13,\n    educ = educ/18\n  )\n\nThe primary variables of interest are our continuous outcome \\(Y=\\) Total Assets, our binary instrument \\(Z=\\) 401(k) Eligibility, and our binary treatment \\(D=\\) 401(k) Participation. The definitions of all the rest of these variables are kind of obscure, and I’m not sure what they all are exactly. However, they define their specification precisely so I’m simply going to mirror what they use in their paper."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#model-specification",
    "href": "posts/causal-inf-high-dim/index.html#model-specification",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Model Specification",
    "text": "Model Specification\n\nThe specification of the full set of controls they construct is defined below as iv_control_spec. It’s a pretty hairy model spec! Let’s create the input matrix and check it’s dimensions.\n\niv_control_spec <- ~ (\n  (\n    marr + twoearn + db + pira + hown + age + I(age^2) + I(age^3)\n    + educ + I(educ^2) + fsize + I(fsize^2)\n  )^2\n  * (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))\n  + (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))^2\n) - 1\niv_X <- sparse.model.matrix(iv_control_spec, data = iv_data)\n\n\n\nN Observations: 9915; N Confounders: 738\n\n\nWhile this isn’t quite the full set of potential controls considered in the paper, it should be close enough to get the point across."
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#first-stage-estimates",
    "href": "posts/causal-inf-high-dim/index.html#first-stage-estimates",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "First-Stage Estimates",
    "text": "First-Stage Estimates\n\nConditional Expected Outcomes\n\nIn order to calculate the LATE in our IV framework, we will estimate the following expected values: \\(E[Y|Z=0,X]\\), \\(E[Y|Z=1,X]\\), \\(E[D|Z=1,X]\\), and \\(E[Z|X]\\) using post-LASSO estimates for each of these values. Let’s get to estimating! As in the paper, we will estimate using LASSO with a pre-determined, data-driven choice of regularization parameter, \\(\\lambda\\).\n\nN <- nrow(iv_X)\nP <- ncol(iv_X)\n\n# Data-driven regularization parameters\nlambda <- 2.2 * sqrt(N) * qnorm(1 - (0.1/log(N))/(2 * (2 * P)))\nlogit_lambda <- lambda/(2 * N)\n\n# Estimate models where instrument == 0\n\n## Outcome Post-LASSO model\nid_z0 <- iv_data$e401 == 0\ney_z0 <- glmnet(x = iv_X[id_z0, ], y = iv_data$tw[id_z0], lambda = lambda)\ney_z0_selected <- names((c <- coef(ey_z0))[(drop(c) != 0), ])\ney_z0_post_data <- cbind(tw = iv_data$tw[id_z0], as.matrix(iv_X[id_z0, ]))\ney_z0_post_form <- paste(\"tw ~\", paste0(ey_z0_selected[-1], collapse = \"+\"))\ney_z0_lm <- lm(formula(ey_z0_post_form), data = as.data.frame(ey_z0_post_data))\n\n## Treatment Post-LASSO model - not needed (E[D] = 0, since D = 1 iff Z = 1)\n\n# Estimate models where instrument == 1\n\n## Outcome Post-LASSO model\nid_z1 <- iv_data$e401 == 1\ney_z1 <- glmnet(x = iv_X[id_z1, ], y = iv_data$tw[id_z1], lambda = lambda)\ney_z1_selected <- names((c <- coef(ey_z1))[(drop(c) != 0), ])\ney_z1_post_data <- cbind(tw = iv_data$tw[id_z1], as.matrix(iv_X[id_z1, ]))\ney_z1_post_form <- paste(\"tw ~\", paste0(ey_z1_selected[-1], collapse = \"+\"))\ney_z1_lm <- lm(formula(ey_z1_post_form), data = as.data.frame(ey_z1_post_data))\n\n## Treatment Post-LASSO model\ned_z1 <- glmnet(\n  x = iv_X[id_z1, ],\n  y = iv_data$p401[id_z1],\n  family = \"binomial\",\n  lambda = logit_lambda\n)\ned_z1_selected <- names((c <- coef(ed_z1))[(drop(c) != 0), ])\ned_z1_post_data <- cbind(p401 = iv_data$p401[id_z1], as.matrix(iv_X[id_z1, ]))\ned_z1_post_form <- paste(\"p401 ~\", paste0(ed_z1_selected[-1], collapse = \"+\"))\ned_z1_lm <- glm(\n  formula(ed_z1_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ed_z1_post_data)\n)\n\n# Estimate instrument as a function of X; Post-LASSO\nez <- glmnet(\n  x = iv_X,\n  y = iv_data$e401,\n  family = \"binomial\",\n  lambda = logit_lambda\n)\nez_selected <- names((c <- coef(ez))[(drop(c) != 0), ])\nez_post_data <- cbind(e401 = iv_data$e401, as.matrix(iv_X))\nez_post_form <- paste(\"e401 ~\", paste0(ez_selected[-1], collapse = \"+\"))\nez_lm <- glm(\n  formula(ez_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ez_post_data)\n)\n\n\n\n\nCalculate LATE\n\nNow, that we’ve estimated models for the expected value of \\(Y\\) and \\(D\\) under the different values of our instrument \\(Z\\), let’s create a data.frame that has the estimated expected values of these variables for every observation. As is standard (and implemented in the paper), we will trim observations to ensure that estimated propensities of our instrument are bounded away from \\({0, 1}\\).\n\nprediction_data <- as.data.frame(as.matrix(iv_X))\niv_expected_values <- data.frame(\n  y = iv_data$tw,\n  d = iv_data$p401,\n  z = iv_data$e401,\n  ey_z0 = predict(ey_z0_lm, prediction_data),\n  ey_z1 = predict(ey_z0_lm, prediction_data),\n  ed_z0 = 0,\n  ed_z1 = predict(ed_z1_lm, prediction_data, type = \"response\"),\n  ez = predict(ez_lm, prediction_data, type = \"response\")\n)\n\n# Trim instrument propensity scores -- No observations are dropped here\niv_expected_values <- iv_expected_values |>\n  filter(ez >= 1e-12 & ez <= (1 - 1e-12))\n\n# Estimate LATE plug-in values\niv_expected_values <- iv_expected_values |>\n  mutate(\n    ay_1 = z*(y - ey_z1)/ez + ey_z1,\n    ay_0 = (1 - z)*(y - ey_z0)/(1 - ez) + ey_z0,\n    ad_1 = z*(d - ed_z1)/ez + ed_z1,\n    ad_0 = 0,\n    LATE = (ay_1 - ay_0)/(ad_1 - ad_0)\n  )\n\n\n\n\nConfidence via Bootstrap\n\nNow that we’ve estimated the plug-in values, let’s calculate the LATE and generate a confidence interval using the described multiplier bootstrap.\n\n# Calculate LATE\nmean_ay_1 <- mean(iv_expected_values$ay_1)\nmean_ay_0 <- mean(iv_expected_values$ay_0)\nmean_ad_1 <- mean(iv_expected_values$ad_1)\nmean_ad_0 <- mean(iv_expected_values$ad_0)\nLATE <- (mean_ay_1 - mean_ay_0)/(mean_ad_1 - mean_ad_0)\n\n# Confidence intervals: both analytic and bootstrap\nanalytic_se <- sqrt(\n  (1/(nrow(iv_expected_values) - 1))\n  * sum(\n    (\n      (iv_expected_values$ay_1 - iv_expected_values$ay_0)\n      /(mean_ad_1 - mean_ad_0)\n      - LATE\n    )^2\n  )\n  /nrow(iv_expected_values)\n)\n\n# Function to generate multiplier weights\nmw <- function(n) {\n  1 + rnorm(n)/sqrt(2) + (rnorm(n)^2 - 1)/2\n}\n\nbootstrap_LATEs <- vapply(\n  1:500,\n  function(i) {\n    weights <- mw(nrow(iv_expected_values))\n    (\n      mean((iv_expected_values$ay_1 - iv_expected_values$ay_0)*weights)\n      /mean((iv_expected_values$ad_1 - iv_expected_values$ad_0)*weights)\n    )\n  },\n  numeric(1)\n)\nbootstrap_se <- (\n  (quantile(bootstrap_LATEs, .75) - quantile(bootstrap_LATEs, .25))\n  / (qnorm(.75) - qnorm(.25))\n)\n\n\n\nLATE: 8391.53 (3305.03) {3098.99}"
  },
  {
    "objectID": "posts/causal-inf-high-dim/index.html#conclusion",
    "href": "posts/causal-inf-high-dim/index.html#conclusion",
    "title": "Program Evaluation and Causal Inference with High-Dimensional Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nAnd voila, we’ve estimated the LATE using IV estimation in a high-dimensional setting! A specific, but very useful, case of this general framework is when we want to directly estimate the effect of a treatment variable that is conditionally exogenous. In that case, we can execute the algorithm shown above, but setting \\(Z = D\\). Other than that, everything is exactly the same.\n\nHDM Package\nIf you want a quick and easy implementation for these methods, check out the hdm package. The package is relatively easy-to-follow, and also works with sparse matrices right out of the box, which is nice. It’s not the most user-friendly package, but it seems to get the job done."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Snippets of Things",
    "section": "",
    "text": "V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins\n\n\n\n\nML\n\n\nCausal Inference\n\n\n\n\nObtain valid inferential statements about a low-dimensional parameter (ATE/LATE) in the presence of potentially high-dimensional confounding using modern ML methods.\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nA. Belloni, V. Chernozhukov, I. Fernandez-Val, C. Hansen\n\n\n\n\nML\n\n\nCausal Inference\n\n\n\n\nEstimators and honest confidence bands for a variety of treatment effects including local average (LATE) and average treatment effects (ATE) in data-rich environments.\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dml/index.html#estimating-the-late-of-401k-participation-on-net-financial-assets",
    "href": "posts/dml/index.html#estimating-the-late-of-401k-participation-on-net-financial-assets",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "Estimating the LATE of 401(k) Participation on Net Financial Assets",
    "text": "Estimating the LATE of 401(k) Participation on Net Financial Assets\n\nWe now will estimate the LATE of 401(k) participation on net financial assets, using 401(k) eligibility as our instrument and 401(k) participation as our treatment.\n\n\nD <- pension$p401\nZ <- pension$e401\n\n\nPartially Linear IV Model\n\nThe following function estimates the partially linear IV model.\n\n\ndml_plivm <- function(X,\n                      Y,\n                      D,\n                      Z,\n                      y_learner,\n                      d_learner,\n                      z_learner = d_learner,\n                      pred_fn_y,\n                      pred_fn_d = pred_fn_y,\n                      pred_fn_z = pred_fn_d) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Convert treatment and instrument to numeric\n    numD <- as.numeric(as.character(D[idx_predict]))\n    numZ <- as.numeric(as.character(Z[idx_predict]))\n    \n    # Outcome prediction model\n    outcome_model <- y_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Residualize outcome\n    outcome_predictions <- pred_fn_y(outcome_model, X[idx_predict, , drop = FALSE])\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_resids <- numD - treatment_predictions\n    \n    # Instrument prediction model\n    instrument_model <- z_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Z[idx_train]\n    )\n    \n    # Residualize instrument\n    instrument_predictions <- pred_fn_z(instrument_model, X[idx_predict, , drop = FALSE])\n    instrument_predictions <- pmin(pmax(instrument_predictions, 0.01), .99)\n    instrument_resids <- numZ - instrument_predictions\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"z_resid\" = unname(instrument_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"z_hat\" = unname(instrument_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n\nLet’s estimate the LATE now!\n\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_plivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_plivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_plivm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  Z = factor(Z),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# TSLS with LASSO residuals\nd_hat_lasso <- lm(d_resid ~ z_resid, first_stage_lasso)$fitted.values\nate_lasso <- lm_robust(first_stage_lasso$y_resid ~ d_hat_lasso, se_type = \"HC1\")\n\n# TSLS with LASSO residuals\nd_hat_enet <- lm(d_resid ~ z_resid, first_stage_enet)$fitted.values\nate_enet <- lm_robust(first_stage_enet$y_resid ~ d_hat_enet, se_type = \"HC1\")\n\n# TSLS with RF residuals\nd_hat_rf <- lm(d_resid ~ z_resid, first_stage_rf)$fitted.values\nate_rf <- lm_robust(first_stage_rf$y_resid ~ d_hat_rf, se_type = \"HC1\")\n\n\n\n\n\n\n\nLASSO\nElasticNet\nRandomForest\n\n\n\n\nEstimate\n13982.593\n13809.441\n12781.930\n\n\nStd. Error\n1980.323\n1977.618\n1938.429\n\n\nRMSE Y\n53280.924\n53123.786\n56275.848\n\n\nRMSE D\n0.415\n0.414\n0.418\n\n\nRMSE Z\n0.444\n0.444\n0.449\n\n\n\n\n\n\n\nInteractive IV Model\n\nFinally, the following function estimates the interactive IV model.\n\n\ndml_iivm <- function(X,\n                     Y,\n                     D,\n                     Z,\n                     y_learner,\n                     d_learner,\n                     z_learner = d_learner,\n                     pred_fn_y,\n                     pred_fn_d = pred_fn_y,\n                     pred_fn_z = pred_fn_d,\n                     always_takers = TRUE,\n                     never_takers = TRUE) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Convert treatment and instrument to numeric\n    numD <- as.numeric(as.character(D))\n    numZ <- as.numeric(as.character(Z))\n    \n    # Outcome model\n    XZ <- cbind(\"Z\" = numZ, X)\n    outcome_model <- y_learner(\n      x = XZ[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Outcome predictions under Instrument == 0\n    XZ[, \"Z\"] <- 0\n    outcome_preds_z0 <- pred_fn_y(outcome_model, XZ[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions under Instrument == 1\n    XZ[, \"Z\"] <- 1\n    outcome_preds_z1 <- pred_fn_y(outcome_model, XZ[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions and residuals\n    outcome_predictions <- (\n      numZ[idx_predict]*outcome_preds_z1\n      + (1 - numZ[idx_predict])*outcome_preds_z0\n    )\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment model\n    XZ <- cbind(\"Z\" = numZ, X)\n    treatment_model <- d_learner(\n      x = XZ[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Treatment predictions under Instrument == 0\n    if (always_takers == FALSE) {\n      treatment_preds_z0 <- rep(0, length(D[idx_predict]))\n    } else {\n      XZ[, \"Z\"] <- 0\n      treatment_preds_z0 <- pred_fn_d(treatment_model, XZ[idx_predict, , drop = FALSE])\n    }\n    \n    # Treatment predictions under Instrument == 1\n    if (never_takers == FALSE) {\n      treatment_preds_z1 <- rep(1, length(D[idx_predict]))\n    } else {\n      XZ[, \"Z\"] <- 1\n      treatment_preds_z1 <- pred_fn_d(treatment_model, XZ[idx_predict, , drop = FALSE])\n    }\n    \n    # Treatment predictions and residuals\n    treatment_predictions <- (\n      numZ[idx_predict]*treatment_preds_z1\n      + (1 - numZ[idx_predict])*treatment_preds_z0\n    )\n    treatment_resids <- numD[idx_predict] - treatment_predictions\n    \n    # Instrument prediction model\n    instrument_model <- z_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Z[idx_train]\n    )\n    \n    # Instrument predictions and residuals\n    instrument_predictions <- pred_fn_z(instrument_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    instrument_resids <- numZ[idx_predict] - instrument_predictions\n    \n    # Construct effect estimates\n    effect_estimates <- (\n      (outcome_preds_z1 - outcome_preds_z0)\n      + (numZ[idx_predict]*(Y[idx_predict] - outcome_preds_z1))\n        /instrument_predictions\n      - ((1 - numZ[idx_predict])*(Y[idx_predict] - outcome_preds_z0))\n        /(1 - instrument_predictions)\n      - (\n        (treatment_preds_z1 - treatment_preds_z0)\n        + (numZ[idx_predict]*(numD[idx_predict] - treatment_preds_z1))\n          /instrument_predictions\n        - ((1 - numZ[idx_predict])*(numD[idx_predict] - treatment_preds_z0))\n          /(1 - instrument_predictions)\n      )\n    )\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"effect_estimates\" = unname(effect_estimates),\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"z_resid\" = unname(instrument_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"z_hat\" = unname(instrument_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n\nLet’s estimate the LATE now!\n\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_iivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso,\n  always_takers = FALSE\n)\nfirst_stage_enet <- dml_iivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet,\n  always_takers = FALSE\n)\nfirst_stage_rf <- dml_iivm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  Z = factor(Z),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d,\n  always_takers = FALSE\n)\n\n# ATEs\nate_lasso <- lm_robust(first_stage_lasso$effect_estimates ~ 1, se_type = \"HC1\")\nate_enet <- lm_robust(first_stage_enet$effect_estimates ~ 1, se_type = \"HC1\")\nate_rf <- lm_robust(first_stage_rf$effect_estimates ~ 1, se_type = \"HC1\")\n\n\n\n\n\n\n\nLASSO\nElasticNet\nRandomForest\n\n\n\n\nEstimate\n9155.272\n8686.631\n7687.822\n\n\nStd. Error\n1458.567\n1323.662\n1213.721\n\n\nRMSE Y\n53083.322\n52912.153\n55839.966\n\n\nRMSE D\n0.275\n0.275\n0.277\n\n\nRMSE Z\n0.444\n0.444\n0.449"
  },
  {
    "objectID": "posts/dml/index.html#packages",
    "href": "posts/dml/index.html#packages",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "Packages",
    "text": "Packages\n\nlibrary(estimatr)\nlibrary(glmnet)\nlibrary(hdm)\nlibrary(ranger)"
  },
  {
    "objectID": "posts/dml/index.html#data",
    "href": "posts/dml/index.html#data",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "Data",
    "text": "Data\n\nFor clarity, we’ll use one of the same empirical examples used in the actual paper. This example is described as follows:\n\nWe use the DML method to estimate the effect of 401(k) eligibility, the treatment variable, and 401(k) participation, a self‐selected decision to receive the treatment that we instrument for with assignment to the treatment state, on accumulated assets. In this example, the treatment variable is not randomly assigned and we aim to eliminate the potential biases due to the lack of random assignment by flexibly controlling for a rich set of variables.\n\n\n\ndata(pension)\npension <- pension[sample(1:nrow(pension), nrow(pension)), ]\n\n# Construct model inputs\nform <- ~ (\n  poly(age, 2) + poly(inc, 2) + poly(educ, 2) + poly(fsize,2)\n  + as.factor(marr) + as.factor(twoearn) + as.factor(db) + as.factor(pira)\n  + as.factor(hown)\n)^2\nX <- model.matrix(form, data = pension)\nX_RF <- pension[, c(\"age\", \"inc\", \"fsize\", \"educ\", \"db\", \"marr\", \"twoearn\", \"pira\", \"hown\")]\nD <- pension$e401\nY <- pension$net_tfa\n\n\nNow, let’s define a few simple functions for applying three different ML models (Random Forest, LASSO, and ElasticNet with \\(\\alpha = 0.5\\)) to our data. These three models will be used across each of the four DML estimation strategies.\n\n\n# LASSO learner and prediction functions\ny_learner_lasso <- \\(x, y) cv.glmnet(x, y, nfolds = 5)\nd_learner_lasso <- \\(x, y) cv.glmnet(x, y, nfolds = 5, family = \"binomial\")\npred_fn_lasso <- \\(mod, data) predict(mod, data,\"lambda.min\", \"response\")\n\n# ElasticNet learner and prediction functions\ny_learner_enet <- \\(x, y) cv.glmnet(x, y, nfolds = 5, alpha = 0.5)\nd_learner_enet <- \\(x, y) cv.glmnet(x, y, nfolds = 5, alpha = 0.5, family = \"binomial\")\npred_fn_enet <- \\(mod, data) predict(mod, data, \"lambda.min\", \"response\")\n\n# Random Forest learner and prediction functions\ny_learner_rf <- \\(x, y) ranger(x = x, y = y, num.trees = 1000)\nd_learner_rf <- \\(x, y) ranger(x = x, y = y, num.trees = 1000, probability = TRUE)\npred_fn_rf_y <- \\(mod, data) predict(mod, data)$predictions\npred_fn_rf_d <- \\(mod, data) predict(mod, data)$predictions[, \"1\"]"
  },
  {
    "objectID": "posts/dml/index.html#conclusion",
    "href": "posts/dml/index.html#conclusion",
    "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
    "section": "Conclusion",
    "text": "Conclusion\n\nAnd that’s it! There are some additional tidbits, for example estimating the ATTE instead of the ATE, estimating confidence intervals with a multiplier bootstrap, and dealing with multiple treatments, but this has covered (IMO) the essentials. This really helped me solidify the underlying estimation strategy, which I found very helpful."
  }
]