{
  "hash": "6cd1d5cff36cf6fc4a96130bd0e6d062",
  "result": {
    "markdown": "---\ntitle: \"Double/Debiased Machine Learning for Treatment and Structural Parameters\"\nsubtitle: \"V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins\"\ndescription: \"Obtain valid inferential statements about a low-dimensional parameter (ATE/LATE) in the presence of potentially high-dimensional confounding using modern ML methods.\"\ndate: \"2022-11-23\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    code-overflow: scroll\n    theme: sandstone\n    highlight: espresso\ncategories: [ML, Causal Inference]\nimage: \"./thumbnail.jpeg\"\n---\n\n::: {.cell}\n<style type=\"text/css\">\n.justify {\n  text-align: justify !important\n}\n</style>\n:::\n\n\n::: {.justify}\n\nDouble/Debiased Machine Learning, proposed by\n[Chernozhukov et al.](https://academic.oup.com/ectj/article/21/1/C1/5056401),\ntackles the problem of inference on some\nparameter, $\\theta_0$, in the setting where there is a potentially\nhigh-dimensional set of control variables/confounders. To do so, this method\nharnesses generic ML algorithms which perform well in high-dimensional settings.\nDML can be applied to learn the ATE and LATE in a partially linear regression\nmodel and partially linear instrumental variables model, respectively. It\ncan also be used to estimate the ATE and LATE in the interactive model and\ninteractive instrumental variables model, where treatment effects are fully\nheterogeneous.\n\nThis post is going to show how to implement estimation of the ATE and LATE\nin both the partially linear and interactive settings. This will be a pretty\nminimal prototype, implemented entirely in base R. I'll use a couple other\npackages for the ML methods, but which models you choose to use are entirely\nseparate from the general method outlined here.\n\n:::\n\n## Packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(estimatr)\nlibrary(glmnet)\nlibrary(hdm)\nlibrary(ranger)\n```\n:::\n\n\n## Data\n\n::: {.justify}\n\nFor clarity, we'll use one of the same\n[empirical examples](https://academic.oup.com/ectj/article/21/1/C1/5056401#130274213)\nused in the actual paper. This example is described as follows:\n\n> We use the DML method to estimate the effect of 401(k) eligibility, the \ntreatment variable, and 401(k) participation, a self‐selected decision to\nreceive the treatment that we instrument for with assignment to the treatment\nstate, on accumulated assets. In this example, the treatment variable is not\nrandomly assigned and we aim to eliminate the potential biases due to the lack\nof random assignment by flexibly controlling for a rich set of variables.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(pension)\npension <- pension[sample(1:nrow(pension), nrow(pension)), ]\n\n# Construct model inputs\nform <- ~ (\n  poly(age, 2) + poly(inc, 2) + poly(educ, 2) + poly(fsize,2)\n  + as.factor(marr) + as.factor(twoearn) + as.factor(db) + as.factor(pira)\n  + as.factor(hown)\n)^2\nX <- model.matrix(form, data = pension)\nX_RF <- pension[, c(\"age\", \"inc\", \"fsize\", \"educ\", \"db\", \"marr\", \"twoearn\", \"pira\", \"hown\")]\nD <- pension$e401\nY <- pension$net_tfa\n```\n:::\n\n\n::: {.justify}\n\nNow, let's define a few simple functions for applying three different ML models\n(Random Forest, LASSO, and ElasticNet with $\\alpha = 0.5$) to our data. These\nthree models will be used across each of the four DML estimation strategies.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LASSO learner and prediction functions\ny_learner_lasso <- \\(x, y) cv.glmnet(x, y, nfolds = 5)\nd_learner_lasso <- \\(x, y) cv.glmnet(x, y, nfolds = 5, family = \"binomial\")\npred_fn_lasso <- \\(mod, data) predict(mod, data,\"lambda.min\", \"response\")\n\n# ElasticNet learner and prediction functions\ny_learner_enet <- \\(x, y) cv.glmnet(x, y, nfolds = 5, alpha = 0.5)\nd_learner_enet <- \\(x, y) cv.glmnet(x, y, nfolds = 5, alpha = 0.5, family = \"binomial\")\npred_fn_enet <- \\(mod, data) predict(mod, data, \"lambda.min\", \"response\")\n\n# Random Forest learner and prediction functions\ny_learner_rf <- \\(x, y) ranger(x = x, y = y, num.trees = 1000)\nd_learner_rf <- \\(x, y) ranger(x = x, y = y, num.trees = 1000, probability = TRUE)\npred_fn_rf_y <- \\(mod, data) predict(mod, data)$predictions\npred_fn_rf_d <- \\(mod, data) predict(mod, data)$predictions[, \"1\"]\n```\n:::\n\n\n## Estimating the ATE of 401(k) Eligibility on Net Financial Assets\n\n::: {.justify}\n\n> In the example in this paper, we use the same data as in\n[Chernozhukov and Hansen (2004)](http://dx.doi.org/10.1162/0034653041811734).\nWe use net financial assets – defined as the sum of IRA balances, 401(k)\nbalances, checking accounts, US saving bonds, other interest‐earning \naccounts in banks and other financial institutions, other \ninterest‐earning assets (such as bonds held personally), stocks, and mutual \nfunds less non‐mortgage debt – as the outcome variable, Y, in our analysis. \nOur treatment variable, D, is an indicator for being eligible to enroll in a \n401(k) plan. The vector of raw covariates, X, consists of age, income, \nfamily size, years of education, a married indicator, a two‐earner status \nindicator, a defined benefit pension status indicator, an IRA participation \nindicator, and a home‐ownership indicator.\n\n:::\n\n### Partially Linear Regression Model\n\n::: {.justify}\n\nFirst we will calculate the ATE of 401(k) eligibility on net financial\nassets in the partially linear model.\n\nThe following function implements DML for the partially linear model.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndml_plm <- function(X, Y, D, y_learner, d_learner, pred_fn_y, pred_fn_d = pred_fn_y) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Outcome prediction model\n    outcome_model <- y_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Residualize outcome\n    outcome_predictions <- pred_fn_y(outcome_model, X[idx_predict, , drop = FALSE])\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    treatment_resids <- (\n      # This is necessary to deal with cases where D is a factor\n      as.numeric(as.character(D[idx_predict])) - treatment_predictions\n    )\n\n    # Collect residuals\n    new_resids <- data.frame(\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n```\n:::\n\n\n::: {.justify}\n\nNow that we've defined the function, let's estimate the ATE using our three\ndifferent ML methods.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_plm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_plm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_plm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# Estimate the ATE using OLS\nate_lasso <- lm_robust(y_resid ~ d_resid, first_stage_lasso, se_type = \"HC1\")\nate_enet <- lm_robust(y_resid ~ d_resid, first_stage_enet, se_type = \"HC1\")\nate_rf <- lm_robust(y_resid ~ d_resid, first_stage_rf, se_type = \"HC1\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|           |     LASSO| ElasticNet| RandomForest|\n|:----------|---------:|----------:|------------:|\n|Estimate   |  9738.496|   9816.064|     8986.493|\n|Std. Error |  1372.968|   1412.982|     1274.455|\n|RMSE Y     | 53255.882|  53939.421|    54589.826|\n|RMSE D     |     0.444|      0.444|        0.447|\n:::\n:::\n\n\n### Interactive Regression Model\n\n::: {.justify}\n\nNow, we'll define the function to estimate the ATE in the fully heterogeneous\nmodel.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndml_irm <- function(X,\n                    Y,\n                    D,\n                    y_learner,\n                    d_learner,\n                    pred_fn_y,\n                    pred_fn_d = pred_fn_y) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    numD <- as.numeric(as.character(D))\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    treatment_resids <- (numD[idx_predict] - treatment_predictions)\n    \n    # Outcome prediction model\n    XD <- cbind(\"D\" = numD, X)\n    outcome_model <- y_learner(\n      x = XD[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Outcome predictions under control\n    XD[, \"D\"] <- 0\n    outcome_preds_d0 <- pred_fn_y(outcome_model, XD[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions under treatment\n    XD[, \"D\"] <- 1\n    outcome_preds_d1 <- pred_fn_y(outcome_model, XD[idx_predict, , drop = FALSE])\n    outcome_predictions <- (\n      numD[idx_predict]*outcome_preds_d1\n      + (1 - numD[idx_predict])*outcome_preds_d0\n    )\n    \n    # Calculate individual level weights\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    inv_prop_scores <- (\n      numD[idx_predict]/treatment_predictions\n      - (1 - numD[idx_predict])/(1 - treatment_predictions)\n    )\n    expected_diff <- outcome_preds_d1 - outcome_preds_d0\n    effect_estimates <- expected_diff + inv_prop_scores*outcome_resids\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"effect_estimates\" = unname(effect_estimates),\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n```\n:::\n\n\nLet's estimate the ATE!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_irm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_irm(\n  X = X,\n  Y = Y,\n  D = D,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_irm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# Estimate the ATE using OLS\nate_lasso <- lm_robust(effect_estimates ~ 1, first_stage_lasso, se_type = \"HC1\")\nate_enet <- lm_robust(effect_estimates ~ 1, first_stage_enet, se_type = \"HC1\")\nate_rf <- lm_robust(effect_estimates ~ 1, first_stage_rf, se_type = \"HC1\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|           |     LASSO| ElasticNet| RandomForest|\n|:----------|---------:|----------:|------------:|\n|Estimate   |  9078.850|   9311.623|     8100.171|\n|Std. Error |  1412.741|   1462.927|     1149.504|\n|RMSE Y     | 53085.113|  53732.086|    54427.880|\n|RMSE D     |     0.444|      0.444|        0.447|\n:::\n:::\n\n\n## Estimating the LATE of 401(k) Participation on Net Financial Assets\n\n::: {.justify}\n\nWe now will estimate the LATE of 401(k) participation on net financial assets,\nusing 401(k) eligibility as our instrument and 401(k) participation as our\ntreatment.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- pension$p401\nZ <- pension$e401\n```\n:::\n\n\n### Partially Linear IV Model\n\n::: {.justify}\n\nThe following function estimates the partially linear IV model.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndml_plivm <- function(X,\n                      Y,\n                      D,\n                      Z,\n                      y_learner,\n                      d_learner,\n                      z_learner = d_learner,\n                      pred_fn_y,\n                      pred_fn_d = pred_fn_y,\n                      pred_fn_z = pred_fn_d) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Convert treatment and instrument to numeric\n    numD <- as.numeric(as.character(D[idx_predict]))\n    numZ <- as.numeric(as.character(Z[idx_predict]))\n    \n    # Outcome prediction model\n    outcome_model <- y_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Residualize outcome\n    outcome_predictions <- pred_fn_y(outcome_model, X[idx_predict, , drop = FALSE])\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment prediction model\n    treatment_model <- d_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Residualize treatment\n    treatment_predictions <- pred_fn_d(treatment_model, X[idx_predict, , drop = FALSE])\n    treatment_resids <- numD - treatment_predictions\n    \n    # Instrument prediction model\n    instrument_model <- z_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Z[idx_train]\n    )\n    \n    # Residualize instrument\n    instrument_predictions <- pred_fn_z(instrument_model, X[idx_predict, , drop = FALSE])\n    instrument_predictions <- pmin(pmax(instrument_predictions, 0.01), .99)\n    instrument_resids <- numZ - instrument_predictions\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"z_resid\" = unname(instrument_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"z_hat\" = unname(instrument_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n```\n:::\n\n\nLet's estimate the LATE now!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_plivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso\n)\nfirst_stage_enet <- dml_plivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet\n)\nfirst_stage_rf <- dml_plivm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  Z = factor(Z),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d\n)\n\n# TSLS with LASSO residuals\nd_hat_lasso <- lm(d_resid ~ z_resid, first_stage_lasso)$fitted.values\nate_lasso <- lm_robust(first_stage_lasso$y_resid ~ d_hat_lasso, se_type = \"HC1\")\n\n# TSLS with LASSO residuals\nd_hat_enet <- lm(d_resid ~ z_resid, first_stage_enet)$fitted.values\nate_enet <- lm_robust(first_stage_enet$y_resid ~ d_hat_enet, se_type = \"HC1\")\n\n# TSLS with RF residuals\nd_hat_rf <- lm(d_resid ~ z_resid, first_stage_rf)$fitted.values\nate_rf <- lm_robust(first_stage_rf$y_resid ~ d_hat_rf, se_type = \"HC1\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|           |     LASSO| ElasticNet| RandomForest|\n|:----------|---------:|----------:|------------:|\n|Estimate   | 13982.593|  13809.441|    12781.930|\n|Std. Error |  1980.323|   1977.618|     1938.429|\n|RMSE Y     | 53280.924|  53123.786|    56275.848|\n|RMSE D     |     0.415|      0.414|        0.418|\n|RMSE Z     |     0.444|      0.444|        0.449|\n:::\n:::\n\n\n### Interactive IV Model\n\n::: {.justify}\n\nFinally, the following function estimates the interactive IV model.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndml_iivm <- function(X,\n                     Y,\n                     D,\n                     Z,\n                     y_learner,\n                     d_learner,\n                     z_learner = d_learner,\n                     pred_fn_y,\n                     pred_fn_d = pred_fn_y,\n                     pred_fn_z = pred_fn_d,\n                     always_takers = TRUE,\n                     never_takers = TRUE) {\n  \n  # Create folds for sample splitting\n  fold1 <- sample(1:nrow(X), size = round((1/3)*nrow(X)))\n  fold2 <- sample((1:nrow(X))[-fold1], size = round((1/3)*nrow(X)))\n  fold3 <- (1:nrow(X))[-c(fold1, fold2)]\n  \n  # Create data.frame to store residuals\n  resids <- data.frame()\n  \n  # Generate outcome model across folds\n  for (fold in list(fold1, fold2, fold3)) {\n    \n    # Get the training/prediction indices\n    idx_train <- (1:nrow(X))[-fold]\n    idx_predict <- fold\n    \n    # Convert treatment and instrument to numeric\n    numD <- as.numeric(as.character(D))\n    numZ <- as.numeric(as.character(Z))\n    \n    # Outcome model\n    XZ <- cbind(\"Z\" = numZ, X)\n    outcome_model <- y_learner(\n      x = XZ[idx_train, , drop = FALSE],\n      y = Y[idx_train]\n    )\n    \n    # Outcome predictions under Instrument == 0\n    XZ[, \"Z\"] <- 0\n    outcome_preds_z0 <- pred_fn_y(outcome_model, XZ[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions under Instrument == 1\n    XZ[, \"Z\"] <- 1\n    outcome_preds_z1 <- pred_fn_y(outcome_model, XZ[idx_predict, , drop = FALSE])\n    \n    # Outcome predictions and residuals\n    outcome_predictions <- (\n      numZ[idx_predict]*outcome_preds_z1\n      + (1 - numZ[idx_predict])*outcome_preds_z0\n    )\n    outcome_resids <- Y[idx_predict] - outcome_predictions\n    \n    # Treatment model\n    XZ <- cbind(\"Z\" = numZ, X)\n    treatment_model <- d_learner(\n      x = XZ[idx_train, , drop = FALSE],\n      y = D[idx_train]\n    )\n    \n    # Treatment predictions under Instrument == 0\n    if (always_takers == FALSE) {\n      treatment_preds_z0 <- rep(0, length(D[idx_predict]))\n    } else {\n      XZ[, \"Z\"] <- 0\n      treatment_preds_z0 <- pred_fn_d(treatment_model, XZ[idx_predict, , drop = FALSE])\n    }\n    \n    # Treatment predictions under Instrument == 1\n    if (never_takers == FALSE) {\n      treatment_preds_z1 <- rep(1, length(D[idx_predict]))\n    } else {\n      XZ[, \"Z\"] <- 1\n      treatment_preds_z1 <- pred_fn_d(treatment_model, XZ[idx_predict, , drop = FALSE])\n    }\n    \n    # Treatment predictions and residuals\n    treatment_predictions <- (\n      numZ[idx_predict]*treatment_preds_z1\n      + (1 - numZ[idx_predict])*treatment_preds_z0\n    )\n    treatment_resids <- numD[idx_predict] - treatment_predictions\n    \n    # Instrument prediction model\n    instrument_model <- z_learner(\n      x = X[idx_train, , drop = FALSE],\n      y = Z[idx_train]\n    )\n    \n    # Instrument predictions and residuals\n    instrument_predictions <- pred_fn_z(instrument_model, X[idx_predict, , drop = FALSE])\n    treatment_predictions <- pmin(pmax(treatment_predictions, 0.01), .99)\n    instrument_resids <- numZ[idx_predict] - instrument_predictions\n    \n    # Construct effect estimates\n    effect_estimates <- (\n      (outcome_preds_z1 - outcome_preds_z0)\n      + (numZ[idx_predict]*(Y[idx_predict] - outcome_preds_z1))\n        /instrument_predictions\n      - ((1 - numZ[idx_predict])*(Y[idx_predict] - outcome_preds_z0))\n        /(1 - instrument_predictions)\n      - (\n        (treatment_preds_z1 - treatment_preds_z0)\n        + (numZ[idx_predict]*(numD[idx_predict] - treatment_preds_z1))\n          /instrument_predictions\n        - ((1 - numZ[idx_predict])*(numD[idx_predict] - treatment_preds_z0))\n          /(1 - instrument_predictions)\n      )\n    )\n    \n    # Collect residuals\n    new_resids <- data.frame(\n      \"effect_estimates\" = unname(effect_estimates),\n      \"y_resid\" = unname(outcome_resids),\n      \"d_resid\" = unname(treatment_resids),\n      \"z_resid\" = unname(instrument_resids),\n      \"y_hat\" = unname(outcome_predictions),\n      \"d_hat\" = unname(treatment_predictions),\n      \"z_hat\" = unname(instrument_predictions),\n      \"idx\" = idx_predict\n    )\n    resids <- do.call(rbind, list(resids, new_resids))\n  }\n  \n  # Return data in original ordering\n  return(resids[order(resids[, \"idx\", drop = TRUE]), , drop = FALSE])\n  \n}\n```\n:::\n\n\nLet's estimate the LATE now!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate DML first stage\nset.seed(123)\nfirst_stage_lasso <- dml_iivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_lasso,\n  d_learner = d_learner_lasso,\n  pred_fn_y = pred_fn_lasso,\n  always_takers = FALSE\n)\nfirst_stage_enet <- dml_iivm(\n  X = X,\n  Y = Y,\n  D = D,\n  Z = Z,\n  y_learner = y_learner_enet,\n  d_learner = d_learner_enet,\n  pred_fn_y = pred_fn_enet,\n  always_takers = FALSE\n)\nfirst_stage_rf <- dml_iivm(\n  X = X_RF,\n  Y = Y,\n  D = factor(D),\n  Z = factor(Z),\n  y_learner = y_learner_rf,\n  d_learner = d_learner_rf,\n  pred_fn_y = pred_fn_rf_y,\n  pred_fn_d = pred_fn_rf_d,\n  always_takers = FALSE\n)\n\n# ATEs\nate_lasso <- lm_robust(first_stage_lasso$effect_estimates ~ 1, se_type = \"HC1\")\nate_enet <- lm_robust(first_stage_enet$effect_estimates ~ 1, se_type = \"HC1\")\nate_rf <- lm_robust(first_stage_rf$effect_estimates ~ 1, se_type = \"HC1\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|           |     LASSO| ElasticNet| RandomForest|\n|:----------|---------:|----------:|------------:|\n|Estimate   |  9155.272|   8686.631|     7687.822|\n|Std. Error |  1458.567|   1323.662|     1213.721|\n|RMSE Y     | 53083.322|  52912.153|    55839.966|\n|RMSE D     |     0.275|      0.275|        0.277|\n|RMSE Z     |     0.444|      0.444|        0.449|\n:::\n:::\n\n\n## Conclusion\n\n::: {.justify}\n\nAnd that's it! There are some additional tidbits, for example estimating the\nATTE instead of the ATE, estimating confidence intervals with a multiplier\nbootstrap, and dealing with multiple treatments, but this has covered (IMO) the\nessentials. This really helped me solidify the underlying estimation\nstrategy, which I found very helpful.\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}