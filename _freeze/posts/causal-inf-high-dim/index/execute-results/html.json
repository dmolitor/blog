{
  "hash": "dfa75d04d809b25524a2042899548e7b",
  "result": {
    "markdown": "---\ntitle: \"Program Evaluation and Causal Inference with High-Dimensional Data\"\nsubtitle: \"A. Belloni, V. Chernozhukov, I. Fernandez-Val, C. Hansen\"\ndescription: \"Estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and average treatment effects (ATE) in data-rich environments.\"\ndate: \"2022-11-08\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    code-overflow: scroll\n    theme: sandstone\n    highlight: espresso\ncategories: [ML, Causal Inference]\nimage: \"./thumbnail.png\"\n---\n\n\n## Packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(here)\nlibrary(Matrix)\nlibrary(readr)\n```\n:::\n\n::: {.cell}\n<style type=\"text/css\">\n.justify {\n  text-align: justify !important\n}\n</style>\n:::\n\n\n::: {.justify}\n\nThis will be replicating (to a small degree),\n[Chernozhukov et al.'s work](https://doi.org/10.3982/ECTA12723) on\nestimating causal effects in high-dimensional settings. This work demonstrates\nhow to estimate treatment effects including LATE (in an Instrumental Variables\nsetting) and ATE among others, as well as constructing honest confidence bands.\nThey demonstrate this within a specific empirical setting. This setting is a\nprior study which uses Instrumental Variables estimation to quantify the effect\nof 401(k) participation on accumulated assets. The instrument in this setting is\n401(k) eligibility which is arguably exogenous after conditioning on income and\nother related confounders.\n\n:::\n\n## Study Data\n\n::: {.justify}\n\nFirst, I will import the data that this study is based on.\n\n::: {.cell}\n\n```{.r .cell-code}\niv_data <- read_tsv(here(\"posts/causal-inf-high-dim/data/high-dim-iv.dat\"))\n\n# Variable transformations\niv_data <- iv_data |>\n  mutate(\n    age = (age - 25)/(64 - 25),\n    inc = (inc + 2652)/(242124 + 2652),\n    fsize = fsize/13,\n    educ = educ/18\n  )\n```\n:::\n\n\nThe primary variables of interest are our continuous outcome \n$Y=$ Total Assets, our binary instrument $Z=$ 401(k) Eligibility, and our\nbinary treatment $D=$ 401(k) Participation. The definitions of all the rest of\nthese variables are kind of obscure, and I'm not sure what they all are exactly.\nHowever, they define their specification precisely so I'm simply going to mirror\nwhat they use in their paper.\n\n:::\n\n## Model Specification\n\n::: {.justify}\n\nThe specification of the full set of controls they construct is defined below\nas `iv_control_spec`. It's a pretty hairy model spec! Let's create the input\nmatrix and check it's dimensions.\n\n::: {.cell}\n\n```{.r .cell-code}\niv_control_spec <- ~ (\n  (\n    marr + twoearn + db + pira + hown + age + I(age^2) + I(age^3)\n    + educ + I(educ^2) + fsize + I(fsize^2)\n  )^2\n  * (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))\n  + (i1 + i2 + i3 + i4 + i5 + i6 + i6 + inc + I(inc^2))^2\n) - 1\niv_X <- sparse.model.matrix(iv_control_spec, data = iv_data)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nN Observations: 9915; N Confounders: 738\n```\n:::\n:::\n\n\nWhile this isn't quite the full set of potential controls considered in the\npaper, it should be close enough to get the point across.\n\n:::\n\n## First-Stage Estimates\n\n### Conditional Expected Outcomes\n\n::: {.justify}\n\nIn order to calculate the LATE in our IV framework, we will estimate the\nfollowing expected values: $E[Y|Z=0,X]$, $E[Y|Z=1,X]$, $E[D|Z=1,X]$, and\n$E[Z|X]$ using post-LASSO estimates for each of these values. Let's get to\nestimating! As in the paper, we will estimate using LASSO with a pre-determined,\ndata-driven choice of regularization parameter, $\\lambda$.\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- nrow(iv_X)\nP <- ncol(iv_X)\n\n# Data-driven regularization parameters\nlambda <- 2.2 * sqrt(N) * qnorm(1 - (0.1/log(N))/(2 * (2 * P)))\nlogit_lambda <- lambda/(2 * N)\n\n# Estimate models where instrument == 0\n\n## Outcome Post-LASSO model\nid_z0 <- iv_data$e401 == 0\ney_z0 <- glmnet(x = iv_X[id_z0, ], y = iv_data$tw[id_z0], lambda = lambda)\ney_z0_selected <- names((c <- coef(ey_z0))[(drop(c) != 0), ])\ney_z0_post_data <- cbind(tw = iv_data$tw[id_z0], as.matrix(iv_X[id_z0, ]))\ney_z0_post_form <- paste(\"tw ~\", paste0(ey_z0_selected[-1], collapse = \"+\"))\ney_z0_lm <- lm(formula(ey_z0_post_form), data = as.data.frame(ey_z0_post_data))\n\n## Treatment Post-LASSO model - not needed (E[D] = 0, since D = 1 iff Z = 1)\n\n# Estimate models where instrument == 1\n\n## Outcome Post-LASSO model\nid_z1 <- iv_data$e401 == 1\ney_z1 <- glmnet(x = iv_X[id_z1, ], y = iv_data$tw[id_z1], lambda = lambda)\ney_z1_selected <- names((c <- coef(ey_z1))[(drop(c) != 0), ])\ney_z1_post_data <- cbind(tw = iv_data$tw[id_z1], as.matrix(iv_X[id_z1, ]))\ney_z1_post_form <- paste(\"tw ~\", paste0(ey_z1_selected[-1], collapse = \"+\"))\ney_z1_lm <- lm(formula(ey_z1_post_form), data = as.data.frame(ey_z1_post_data))\n\n## Treatment Post-LASSO model\ned_z1 <- glmnet(\n  x = iv_X[id_z1, ],\n  y = iv_data$p401[id_z1],\n  family = \"binomial\",\n  lambda = logit_lambda\n)\ned_z1_selected <- names((c <- coef(ed_z1))[(drop(c) != 0), ])\ned_z1_post_data <- cbind(p401 = iv_data$p401[id_z1], as.matrix(iv_X[id_z1, ]))\ned_z1_post_form <- paste(\"p401 ~\", paste0(ed_z1_selected[-1], collapse = \"+\"))\ned_z1_lm <- glm(\n  formula(ed_z1_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ed_z1_post_data)\n)\n\n# Estimate instrument as a function of X; Post-LASSO\nez <- glmnet(\n  x = iv_X,\n  y = iv_data$e401,\n  family = \"binomial\",\n  lambda = logit_lambda\n)\nez_selected <- names((c <- coef(ez))[(drop(c) != 0), ])\nez_post_data <- cbind(e401 = iv_data$e401, as.matrix(iv_X))\nez_post_form <- paste(\"e401 ~\", paste0(ez_selected[-1], collapse = \"+\"))\nez_lm <- glm(\n  formula(ez_post_form),\n  family = \"binomial\",\n  data = as.data.frame(ez_post_data)\n)\n```\n:::\n\n\n:::\n\n### Calculate LATE\n\n::: {.justify}\n\nNow, that we've estimated models for the expected value of $Y$ and $D$ under\nthe different values of our instrument $Z$, let's create a data.frame that\nhas the estimated expected values of these variables for every observation. As\nis standard (and implemented in the paper), we will trim observations to ensure\nthat estimated propensities of our instrument are bounded away from ${0, 1}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_data <- as.data.frame(as.matrix(iv_X))\niv_expected_values <- data.frame(\n  y = iv_data$tw,\n  d = iv_data$p401,\n  z = iv_data$e401,\n  ey_z0 = predict(ey_z0_lm, prediction_data),\n  ey_z1 = predict(ey_z0_lm, prediction_data),\n  ed_z0 = 0,\n  ed_z1 = predict(ed_z1_lm, prediction_data, type = \"response\"),\n  ez = predict(ez_lm, prediction_data, type = \"response\")\n)\n\n# Trim instrument propensity scores -- No observations are dropped here\niv_expected_values <- iv_expected_values |>\n  filter(ez >= 1e-12 & ez <= (1 - 1e-12))\n\n# Estimate LATE plug-in values\niv_expected_values <- iv_expected_values |>\n  mutate(\n    ay_1 = z*(y - ey_z1)/ez + ey_z1,\n    ay_0 = (1 - z)*(y - ey_z0)/(1 - ez) + ey_z0,\n    ad_1 = z*(d - ed_z1)/ez + ed_z1,\n    ad_0 = 0,\n    LATE = (ay_1 - ay_0)/(ad_1 - ad_0)\n  )\n```\n:::\n\n\n:::\n\n### Confidence via Bootstrap\n\n::: {.justify}\n\nNow that we've estimated the plug-in values, let's calculate the LATE and\ngenerate a confidence interval using the described multiplier bootstrap.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate LATE\nmean_ay_1 <- mean(iv_expected_values$ay_1)\nmean_ay_0 <- mean(iv_expected_values$ay_0)\nmean_ad_1 <- mean(iv_expected_values$ad_1)\nmean_ad_0 <- mean(iv_expected_values$ad_0)\nLATE <- (mean_ay_1 - mean_ay_0)/(mean_ad_1 - mean_ad_0)\n\n# Confidence intervals: both analytic and bootstrap\nanalytic_se <- sqrt(\n  (1/(nrow(iv_expected_values) - 1))\n  * sum(\n    (\n      (iv_expected_values$ay_1 - iv_expected_values$ay_0)\n      /(mean_ad_1 - mean_ad_0)\n      - LATE\n    )^2\n  )\n  /nrow(iv_expected_values)\n)\n\n# Function to generate multiplier weights\nmw <- function(n) {\n  1 + rnorm(n)/sqrt(2) + (rnorm(n)^2 - 1)/2\n}\n\nbootstrap_LATEs <- vapply(\n  1:500,\n  function(i) {\n    weights <- mw(nrow(iv_expected_values))\n    (\n      mean((iv_expected_values$ay_1 - iv_expected_values$ay_0)*weights)\n      /mean((iv_expected_values$ad_1 - iv_expected_values$ad_0)*weights)\n    )\n  },\n  numeric(1)\n)\nbootstrap_se <- (\n  (quantile(bootstrap_LATEs, .75) - quantile(bootstrap_LATEs, .25))\n  / (qnorm(.75) - qnorm(.25))\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nLATE: 8391.53 (3305.03) {3098.99}\n```\n:::\n:::\n\n\n:::\n\n## Conclusion\n\n::: {.justify}\n\nAnd voila, we've estimated the LATE using IV estimation in a high-dimensional\nsetting! A specific, but very useful, case of this general framework is\nwhen we want to directly estimate the effect of a treatment variable that\nis conditionally exogenous. In that case, we can execute the algorithm shown\nabove, but setting $Z = D$. Other than that, everything is exactly the same.\n\n### HDM Package\nIf you want a quick and easy implementation for these methods, check out the\n[`hdm` package](https://cran.r-project.org/web/packages/hdm/index.html). The\npackage is relatively easy-to-follow, and also works with sparse matrices right\nout of the box, which is nice. It's not the most user-friendly package, but it\nseems to get the job done.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}